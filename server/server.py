from fastapi import FastAPI, UploadFile, File
import uvicorn
import cv2
import numpy as np
from ultralytics import YOLOWorld
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import io
import torch
from fastapi import FastAPI, UploadFile, File
import uvicorn
import cv2
import numpy as np
from ultralytics import YOLOWorld
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import io
import torch
import time

app = FastAPI()

print("=========================================")
print("â³ [1/2] YOLO-World ë¡œë”© ì¤‘... (ì†, í•¸ë“œí°)")
yolo_model = YOLOWorld('yolov8s-world.pt')
yolo_model.set_classes(["hand", "cell phone"])

print("â³ [2/2] VLM (Qwen2-VL) ë¡œë”© ì¤‘... (ì‹œê°„ ì†Œìš”ë¨)")
vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct", 
    torch_dtype="auto", 
    device_map="auto"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! (8000ë²ˆ í¬íŠ¸ ëŒ€ê¸° ì¤‘)")
print("=========================================")

@app.post("/detect")
def detect_object(user_id: str = "Unknown", file: UploadFile = File(...)):
    # 1. ì´ë¯¸ì§€ ì½ê¸°
    file_bytes = file.file.read()
    nparr = np.frombuffer(file_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    # 2. ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì •
    start = time.time()
    results = yolo_model.predict(img, conf=0.1, verbose=False)
    duration = (time.time() - start) * 1000
    
    # ë¡œê·¸ ì¶œë ¥ (ì‚¬ìš©ì ID í¬í•¨)
    print(f"âš¡ [{user_id}] YOLO ìš”ì²­ ì²˜ë¦¬: {duration:.2f} ms")

    # 3. ê²°ê³¼ í¬ì¥
    detections = []
    for box in results[0].boxes:
        bbox = box.xywh[0].tolist() # ì¤‘ì‹¬x, ì¤‘ì‹¬y, ë„ˆë¹„, ë†’ì´
        label = yolo_model.names[int(box.cls[0])]
        detections.append({"label": label, "bbox": bbox})
    
    return {
        "detections": detections, 
        "server_time": duration 
    }

@app.post("/analyze")
def analyze_image(user_id: str = "Unknown", file: UploadFile = File(...)):
    print(f"\nğŸ¤– [{user_id}] VLM ì •ë°€ ë¶„ì„ ìš”ì²­ ë„ì°©!")
    try:
        image = Image.open(io.BytesIO(file.file.read()))
        prompt = "Describe this scene in detail, focusing on what the person is doing with their hands."
        messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": prompt}]}]
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], images=process_vision_info(messages)[0], padding=True, return_tensors="pt").to(vlm_model.device)
        
        start = time.time()
        generated_ids = vlm_model.generate(**inputs, max_new_tokens=100)
        duration = (time.time() - start) * 1000
        
        result = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].split("assistant\n")[-1].strip()
        
        print(f"ğŸ“ [{user_id}] ë¶„ì„ ì™„ë£Œ ({duration:.2f} ms): {result[:30]}...")
        return {"result": result}
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return {"result": "ë¶„ì„ ì‹¤íŒ¨"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
