from fastapi import FastAPI, UploadFile, File
import uvicorn
import cv2
import numpy as np
from ultralytics import YOLOWorld
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import io
import torch
from fastapi import FastAPI, UploadFile, File
import uvicorn
import cv2
import numpy as np
from ultralytics import YOLOWorld
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import io
import torch
import time

app = FastAPI()

print("=========================================")
print("â³ [1/2] YOLO-World ë¡œë”© ì¤‘... (ì†, í•¸ë“œí°)")
yolo_model = YOLOWorld('yolov8s-world.pt')
yolo_model.set_classes(["hand", "cell phone"])

print("â³ [2/2] VLM (Qwen2-VL) ë¡œë”© ì¤‘... (ì‹œê°„ ì†Œìš”ë¨)")
vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct", 
    torch_dtype="auto", 
    device_map="auto"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ! (8000ë²ˆ í¬íŠ¸ ëŒ€ê¸° ì¤‘)")
print("=========================================")

@app.post("/detect")
async def detect_object(user_id: str = "Unknown", file: UploadFile = File(...)):
    # 1. ì´ë¯¸ì§€ ì½ê¸°
    file_bytes = await file.read()
    nparr = np.frombuffer(file_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    # 2. ì¶”ë¡  ë° ì‹œê°„ ì¸¡ì •
    start = time.time()
    results = yolo_model.predict(img, conf=0.1, verbose=False)
    duration = (time.time() - start) * 1000
    
    # ë¡œê·¸ ì¶œë ¥ (ì‚¬ìš©ì ID í¬í•¨)
    print(f"âš¡ [{user_id}] YOLO ìš”ì²­ ì²˜ë¦¬: {duration:.2f} ms")

    # 3. ê²°ê³¼ í¬ì¥
    detections = []
    for box in results[0].boxes:
        bbox = box.xywh[0].tolist() # ì¤‘ì‹¬x, ì¤‘ì‹¬y, ë„ˆë¹„, ë†’ì´
        label = yolo_model.names[int(box.cls[0])]
        detections.append({"label": label, "bbox": bbox})
    
    return {"detections": detections}

@app.post("/analyze")
async def analyze_image(user_id: str = "Unknown", file: UploadFile = File(...)):
    print(f"\nğŸ¤– [{user_id}] VLM ì •ë°€ ë¶„ì„ ìš”ì²­ ë„ì°©!")
    try:
        image = Image.open(io.BytesIO(await file.read()))
        prompt = "Describe this scene in detail, focusing on what the person is doing with their hands."
        messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": prompt}]}]
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], images=process_vision_info(messages)[0], padding=True, return_tensors="pt").to(vlm_model.device)
        
        start = time.time()
        generated_ids = vlm_model.generate(**inputs, max_new_tokens=100)
        duration = (time.time() - start) * 1000
        
        result = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].split("assistant\n")[-1].strip()
        
        print(f"ğŸ“ [{user_id}] ë¶„ì„ ì™„ë£Œ ({duration:.2f} ms): {result[:30]}...")
        return {"result": result}
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        return {"result": "ë¶„ì„ ì‹¤íŒ¨"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
app = FastAPI()

print("=========================================")
print("â³ [1/3] YOLO-World ëª¨ë¸ ë¡œë”© ì¤‘...")
# 1. YOLO-World ë¡œë“œ
yolo_model = YOLOWorld('yolov8s-world.pt')

# â˜… ë‹¤ì—°ë‹˜ì´ ì›í•˜ì‹œëŠ” íƒì§€ ê°ì²´ ì„¤ì •
target_classes = ["hand", "cell phone"]
yolo_model.set_classes(target_classes)
print(f"âœ… YOLO ì„¤ì • ì™„ë£Œ: {target_classes} íƒì§€ ëª¨ë“œ")

print("â³ [2/3] VLM (Qwen2-VL) ëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ ì¢€ ê±¸ë¦¼)")
# 2. VLM ë¡œë“œ (GPU ê°€ì† í™œì„±í™”)
vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct", 
    torch_dtype="auto", 
    device_map="auto"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
print("âœ… VLM ë¡œë”© ì™„ë£Œ!")
print("ğŸš€ [3/3] ì„œë²„ ê°€ë™ ì‹œì‘! (8000ë²ˆ í¬íŠ¸)")
print("=========================================")

@app.get("/")
def read_root():
    return {"status": "RunPod Server is Running!"}

@app.post("/detect")
async def detect_object(file: UploadFile = File(...)):
    # 1. ì´ë¯¸ì§€ ì½ê¸°
    file_bytes = await file.read()
    nparr = np.frombuffer(file_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

    # 2. ì¶”ë¡  ìˆ˜í–‰ (YOLO)
    # conf=0.1: í™•ì‹ ë„ 10% ì´ìƒì´ë©´ íƒì§€
    results = yolo_model.predict(img, conf=0.1, verbose=False)
    
    # 3. ê²°ê³¼ í¬ì¥ (Clientê°€ ì›í•˜ëŠ” í¬ë§·ìœ¼ë¡œ ë³€í™˜)
    detections = []
    for box in results[0].boxes:
        cls_id = int(box.cls[0])
        label = yolo_model.names[cls_id] # hand, cell phone ë“±
        
        # xywh: ì¤‘ì‹¬x, ì¤‘ì‹¬y, ë„ˆë¹„, ë†’ì´ (Clientì—ì„œ ê³„ì‚°í•˜ê¸° í¸í•˜ê²Œ)
        bbox = box.xywh[0].tolist() 
        
        detections.append({
            "label": label,
            "bbox": bbox,
            "conf": float(box.conf[0])
        })
    
    return {"status": "success", "detections": detections}

@app.post("/analyze")
async def analyze_image(file: UploadFile = File(...)):
    print("ğŸ¤– VLM ë¶„ì„ ìš”ì²­ ìˆ˜ì‹ !")
    try:
        # 1. ì´ë¯¸ì§€ ë³€í™˜ (OpenCV -> PIL)
        image_data = await file.read()
        image = Image.open(io.BytesIO(image_data))

        # 2. ì§ˆë¬¸ ì„¤ì •
        prompt = "Describe this scene in detail, focusing on what the person is doing with their hands."
        
        messages = [{
            "role": "user", 
            "content": [{"type": "image", "image": image}, {"type": "text", "text": prompt}]
        }]

        # 3. ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text], 
            images=image_inputs, 
            videos=video_inputs, 
            padding=True, 
            return_tensors="pt"
        ).to(vlm_model.device)

        # 4. ë‹µë³€ ìƒì„±
        generated_ids = vlm_model.generate(**inputs, max_new_tokens=150)
        output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].split("assistant\n")[-1].strip()
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {output_text[:50]}...")
        return {"status": "success", "result": output_text}
        
    except Exception as e:
        print(f"âŒ VLM ì—ëŸ¬: {e}")
        return {"status": "error", "result": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)