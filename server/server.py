from fastapi import FastAPI, UploadFile, File
import uvicorn
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import io

app = FastAPI()
#http://127.0.0.1:8000/docs
# ==========================================
# [1] AI ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì¼¤ ë•Œ ë”± í•œ ë²ˆ ì‹¤í–‰)
# ==========================================
print("â³ AI ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤... (ì²˜ìŒì—” ì˜¤ë˜ ê±¸ë¦¼)")

# ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (2BëŠ” ê°€ë³ê³  ë¹ ë¦„, ì„±ëŠ¥ì„ ì›í•˜ë©´ 7Bë¡œ ë³€ê²½ ê°€ëŠ¥)
MODEL_ID = "Qwen/Qwen2-VL-2B-Instruct"

try:
    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ GPU ì‚¬ìš©)
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_ID,
        torch_dtype="auto",
        device_map="auto" 
    )
    
    # í”„ë¡œì„¸ì„œ(ì´ë¯¸ì§€ ì²˜ë¦¬ê¸°) ë¶ˆëŸ¬ì˜¤ê¸°
    processor = AutoProcessor.from_pretrained(MODEL_ID)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì´ì œ ë˜‘ë˜‘í•´ì¡ŒìŠµë‹ˆë‹¤.")
    
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("í˜¹ì‹œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ê¼¬ì˜€ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ==========================================
# [2] í†µì‹  API ì •ì˜
# ==========================================

@app.post("/analyze")
async def analyze_image(file: UploadFile = File(...)):
    print(f"ğŸ“¸ ì´ë¯¸ì§€ ìˆ˜ì‹ : {file.filename} -> ë¶„ì„ ì‹œì‘")

    # 1. ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
    image_data = await file.read()
    image = Image.open(io.BytesIO(image_data))

    # 2. AIì—ê²Œ ì§ˆë¬¸í•  ë‚´ìš© (Prompt)
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Describe this image in detail within 2 sentences."} 
                # (í•´ì„: ì´ ì´ë¯¸ì§€ë¥¼ 2ë¬¸ì¥ ì´ë‚´ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì¤˜)
            ],
        }
    ]

    # 3. ì „ì²˜ë¦¬ (Preprocessing)
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    ).to(model.device)

    # 4. ì¶”ë¡  (Inference) - AIê°€ ìƒê°í•˜ëŠ” ì‹œê°„
    generated_ids = model.generate(**inputs, max_new_tokens=128)
    
    # 5. ê²°ê³¼ í•´ì„ (Decoding)
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]

    print(f"ğŸ¤– AI ë‹µë³€: {output_text}")

    return {
        "status": "success",
        "result": output_text
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)